# 降噪与抗锯齿

## 前言
得益于各种层出不穷的降噪算法，使用光线追踪计算全局光照时需要的采样数量大幅减少，大大加快了基于路径追踪（Path Tracing）渲染的收敛速度。然而之前绝大部分的渲染降噪算法所接受的输入图像的噪点大小都相对有限：通常是一副每个像素有几十个甚至几百个光线样本所渲染的图像。这些算法通过处理可以将这些带有噪点的图像还原到和已经收敛的最终渲染结果非常接近的程度。

在实时渲染中，为了达到每秒60帧的渲染速度，每一帧只有16毫秒的时间预算。因此若要在游戏中使用光线追踪求解全局光照，那么每个像素的光线求交和着色的预算都不可能达到“数十个”样本的级别。在每像素只有一个样本时，图像的噪点会非常大，尤其在只被间接光照照亮的区域，基本上每数十个像素中只有非常少量的像素不是黑色的，很难从噪点图像中重建出收敛过的图像。

为了在实时渲染中得到尽可能多的样本数量，由于帧与帧之间具有较明显的连续性，上一帧的某个物体的微小表面在下几帧中仍会出现，因此可以把采样点分布在帧序列里，充分利用之前几帧的采样结果。

为了给噪点非常大的图像降噪，还需要收集许多其他可用信息。实时程序的一个优势就是可以通过传统光栅化管线生成位置、法线等无噪点的GBuffer数据，可以充分利用GBuffer数据对原图像进行降噪。

由于降噪和抗锯齿有不少共通之处，也可以共用大部分资源，可以将降噪和抗锯齿结合，同时实现降噪和抗锯齿的效果。

## 算法目标及需求

* 算法输出的结果在时间维度上稳定，没有帧间抖动等问题。
* 输入图像的噪点可能非常大，对于采样数量很少时也可以输出较好的结果。
* 输出结果不需要保证光照完全正确，但要尽可能地保留着色的细节。
* 算法本身和光线传递的模拟无关，应适用于各种光线追踪的算法。

## 算法流程

* 为了使不同帧在每个像素的不同局部位置采样，需要将采样位置进行微小地偏移。可以向摄像机的投影矩阵加上微小偏移量以影响所有的像素点。
    ```cpp
        ubo.jitterProj[2][0] += (1.0f - 2.0f * SampleX) / width;
        ubo.jitterProj[2][1] += (1.0f - 2.0f * SampleY) / height;
    ```  

* 使用经过偏移的投影矩阵生成GBuffer

* 使用光线追踪渲染出原始图像

* 根据GBuffer判断当前位置是否有物体，仅需处理有物体的区域。

* 判断摄像机是否在移动，若在移动则根据上一帧的观察、投影矩阵计算当期像素位置对应的上一帧的像素位置；若为静止则直接使用当前像素位置。

* 将当前位置的GBuffer数据和上一帧对应位置的GBuffer数据比较，并计算出对于上一帧结果的保留系数。系数越大，则上一帧的输出结果所占权重越大；系数越小，表明当前像素与上一帧对应像素的差异越大，当前像素的权重需要提高。同时，由于投影矩阵添加了抖动，计算所得的物体边界的保留系数也比较小，可随着时间积累提高上一帧图像的权重，当物体或摄像机移动后重新进行时间的积累。

* 设置空间滤波的范围大小，对于保留系数较小的像素，应提高其空间滤波的范围。

* 在进行空间滤波时，为保留纹理细节，可先将光追原始图像除以GBuffer中的纹理颜色，得到像素得照度，对照度进行空间滤波后再将纹理叠加回来，这样即可在模糊时保留纹理信息。

* 对于滤波范围内得每个像素，需根据位置及法线信息，使用合适的方法计算权重，尽可能地保留边缘信息。

* 将空间滤波的结果根据保留系数与上一帧对应像素进行混合，输出结果。

* 在降噪过程之后保存当前帧的GBuffer、输出图像、观察和投影矩阵，作为下一帧的输入。

## Reference

* [Spatiotemporal Variance-Guided Filter, 向实时光线追踪迈进](https://zhuanlan.zhihu.com/p/28288053)
* [引擎搭建记录(2) - 时间性抗锯齿(TAA)](https://zhuanlan.zhihu.com/p/64993622)
* [反走样技术（一）：几何反走样](https://zhuanlan.zhihu.com/p/28800047)